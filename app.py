"""NeuroWriter - Streamlit Web Application (Interactive Self-Evolving Pipeline)"""
import streamlit as st
import logging
from concurrent.futures import ThreadPoolExecutor
from typing import Optional
from core.pipeline_orchestrator import PipelineOrchestrator, MAX_EVOLUTION_ITERATIONS
from core.fact_checker import FactChecker
from core.llm_client import get_llm_client
import config

logger = logging.getLogger(__name__)
logging.basicConfig(level=config.LOG_LEVEL)

# Configure page
st.set_page_config(
    page_title="NeuroWriter",
    page_icon="ğŸ§ ",
    layout="wide",
    initial_sidebar_state="expanded"
)

# Custom CSS
st.markdown("""
<style>
.main {
    padding: 2rem;
}
.stChatMessage {
    background-color: #f0f2f6;
    padding: 1rem;
    border-radius: 0.5rem;
    margin: 0.5rem 0;
}
</style>
""", unsafe_allow_html=True)


# ------------------------------------------------------------------
# Session state initialization
# ------------------------------------------------------------------

def initialize_session():
    """Initialize session state variables"""
    defaults = {
        "history": [],
        "current_topic": None,
        "generation_result": None,
        "fact_check_result": None,
        "chat_messages": [],
        "current_intro": None,
        "current_references": None,
        "collected_papers": None,
        "parsed_topic": None,
        # Pipeline state machine
        "pipeline_state": "IDLE",
        "pipeline_iteration": 0,
        "topic_analysis": None,
        "search_queries": [],
        "paper_pool": [],
        "landscape": {},
        "reference_pool": [],
        "writing_strategy": {},
        "introduction_text": "",
        "evaluation_result": {},
        "iteration_history": [],
        # Orchestrator recreation
        "api_key_stored": "",
        "model_stored": "",
        "provider_stored": "openai",
        "azure_endpoint_stored": "",
        "azure_deployment_stored": "",
        "azure_api_version_stored": "2024-12-01-preview",
        "azure_base_model_stored": "",
        # Additional research UI flag (Issue 2)
        "show_additional_research": False,
        # Self-evolution user participation
        "evolution_auto_mode": False,
        "user_evolution_feedback": "",
        "evolution_claims": [],
        "evolution_completeness_gaps": [],
        "evolution_queries": [],
        "evolution_details": [],
        # Evaluation error handling
        "evaluation_error": None,
        # Topic disambiguation
        "topic_resolutions": {},
        "queries_regenerated": False,
    }
    for key, default in defaults.items():
        if key not in st.session_state:
            st.session_state[key] = default


def reset_pipeline():
    """Reset pipeline state to IDLE"""
    keys = [
        "pipeline_state", "pipeline_iteration", "topic_analysis",
        "search_queries", "paper_pool", "landscape", "reference_pool",
        "writing_strategy", "introduction_text", "evaluation_result",
        "iteration_history", "generation_result", "current_topic",
        "fact_check_result",
        "evolution_auto_mode", "user_evolution_feedback",
        "evolution_claims", "evolution_completeness_gaps",
        "evolution_queries", "evolution_details",
        "evaluation_error", "topic_resolutions", "queries_regenerated",
    ]
    defaults = {
        "pipeline_state": "IDLE",
        "pipeline_iteration": 0,
        "topic_analysis": None,
        "search_queries": [],
        "paper_pool": [],
        "landscape": {},
        "reference_pool": [],
        "writing_strategy": {},
        "introduction_text": "",
        "evaluation_result": {},
        "iteration_history": [],
        "generation_result": None,
        "current_topic": None,
        "fact_check_result": None,
        "show_additional_research": False,
        "evolution_auto_mode": False,
        "user_evolution_feedback": "",
        "evolution_claims": [],
        "evolution_completeness_gaps": [],
        "evolution_queries": [],
        "evolution_details": [],
        "evaluation_error": None,
        "topic_resolutions": {},
        "queries_regenerated": False,
    }
    for k in keys:
        st.session_state[k] = defaults.get(k)
    st.session_state.show_additional_research = False


def get_orchestrator() -> Optional[PipelineOrchestrator]:
    """Create PipelineOrchestrator from stored credentials"""
    api_key = st.session_state.get("api_key_stored", "")
    model = st.session_state.get("model_stored", "gpt-4o")
    provider = st.session_state.get("provider_stored", "openai")

    if not api_key:
        return None

    if provider == "openai" and not api_key.startswith("sk-"):
        return None

    if provider == "azure_openai":
        azure_endpoint = st.session_state.get("azure_endpoint_stored", "")
        if not azure_endpoint:
            return None

    try:
        return PipelineOrchestrator(
            api_key=api_key,
            model=model,
            provider=provider,
            azure_endpoint=st.session_state.get("azure_endpoint_stored") or None,
            api_version=st.session_state.get("azure_api_version_stored") or None,
            base_model=st.session_state.get("azure_base_model_stored") or None,
        )
    except Exception as e:
        logger.error(f"Failed to create orchestrator: {e}")
        return None


# ------------------------------------------------------------------
# Header & sidebar
# ------------------------------------------------------------------

def display_header():
    """Display application header"""
    st.markdown("# ğŸ§  NeuroWriter")
    st.markdown(
        "**EEG/Deep Learning ì˜í•™ë…¼ë¬¸ Introduction Generator**  \n"
        "ë‡ŒíŒŒì™€ ë”¥ëŸ¬ë‹ ê¸°ë°˜ ì‹ ê²½ê³¼/ì •ì‹ ê³¼ ì—°êµ¬ ì£¼ì œë¥¼ ì…ë ¥í•˜ë©´, "
        "PubMed ë…¼ë¬¸ì„ ì¸ìš©í•˜ë©° ì˜ì–´ Introductionì„ ìë™ ìƒì„±í•©ë‹ˆë‹¤."
    )


def setup_sidebar():
    """Setup sidebar configuration"""
    with st.sidebar:
        st.markdown("## âš™ï¸ ì„¤ì •")

        # Provider selection
        provider = st.selectbox(
            "LLM Provider",
            ["OpenAI", "Azure OpenAI"],
            index=0,
        )
        provider_key = "openai" if provider == "OpenAI" else "azure_openai"

        # API Key input (common to both providers)
        api_key = st.text_input(
            "API Key",
            type="password",
            help=(
                "https://platform.openai.com/api-keysì—ì„œ ë°œê¸‰ë°›ìœ¼ì„¸ìš”"
                if provider_key == "openai"
                else "Azure OpenAI ë¦¬ì†ŒìŠ¤ì˜ API í‚¤ë¥¼ ì…ë ¥í•˜ì„¸ìš”"
            ),
        )

        if provider_key == "openai":
            # OpenAI: model selector
            model = st.selectbox(
                "LLM Model",
                ["gpt-4o", "gpt-4o-mini", "gpt-4-turbo", "gpt-5", "gpt-5.2"],
                index=4,
            )
            st.session_state.azure_endpoint_stored = ""
            st.session_state.azure_deployment_stored = ""
            st.session_state.azure_api_version_stored = "2024-12-01-preview"
            st.session_state.azure_base_model_stored = ""
        else:
            # Azure OpenAI: endpoint, deployment, api version, base model
            azure_endpoint = st.text_input(
                "Azure Endpoint",
                placeholder="https://your-resource.openai.azure.com/",
                help="Azure OpenAI ë¦¬ì†ŒìŠ¤ì˜ ì—”ë“œí¬ì¸íŠ¸ URL",
            )
            model = st.text_input(
                "Deployment Name",
                placeholder="gpt-51-deployment",
                help="Azure OpenAI ë°°í¬ ì´ë¦„",
            )
            azure_api_version = st.text_input(
                "API Version",
                value="2024-12-01-preview",
                help="Azure OpenAI API ë²„ì „",
            )
            base_model_option = st.selectbox(
                "Base Model",
                ["gpt-5.1", "gpt-4o", "gpt-4o-mini", "ê¸°íƒ€ (ì§ì ‘ ì…ë ¥)"],
                index=0,
                help="ì‹¤ì œ ë°°í¬ëœ ëª¨ë¸ëª… (reasoning model ê°ì§€ì— ì‚¬ìš©)",
            )
            if base_model_option == "ê¸°íƒ€ (ì§ì ‘ ì…ë ¥)":
                azure_base_model = st.text_input(
                    "Base Model ì´ë¦„",
                    placeholder="gpt-4-turbo",
                )
            else:
                azure_base_model = base_model_option

            st.session_state.azure_endpoint_stored = azure_endpoint
            st.session_state.azure_api_version_stored = azure_api_version
            st.session_state.azure_base_model_stored = azure_base_model
            st.session_state.azure_deployment_stored = model

        # Store for orchestrator recreation
        st.session_state.api_key_stored = api_key
        st.session_state.model_stored = model
        st.session_state.provider_stored = provider_key

        # Reference style
        st.markdown("### ì°¸ê³ ë¬¸í—Œ ìŠ¤íƒ€ì¼")
        reference_style = st.selectbox(
            "Citation Style",
            ["APA", "Vancouver", "AMA"],
            index=0,
            help="APA: Author (Year). Title. Journal.\nVancouver: Number. Author. Title. Journal. Year.\nAMA: Author. Title. Journal. Year;Vol(Issue):Pages."
        )

        # Cache management
        st.markdown("### ìºì‹œ ê´€ë¦¬")
        from utils.cache import PubmedCache
        cache = PubmedCache()
        stats = cache.get_stats()
        st.write(f"ë…¼ë¬¸: {stats['article_count']}ê±´ | ê²€ìƒ‰: {stats['search_count']}ê±´")
        if st.button("ìºì‹œ ì´ˆê¸°í™”", key="clear_cache_btn"):
            cache.clear_cache()
            st.success("ìºì‹œê°€ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤")
            st.rerun()

        # Pipeline state indicator
        state = st.session_state.pipeline_state
        if state != "IDLE":
            st.markdown("### íŒŒì´í”„ë¼ì¸ ìƒíƒœ")
            state_labels = {
                "IDLE": "ëŒ€ê¸°",
                "PARSING": "ì£¼ì œ ë¶„ì„ ì¤‘",
                "CONFIRM_QUERIES": "ì¿¼ë¦¬ í™•ì¸ ëŒ€ê¸°",
                "RESEARCHING": "ë¦¬ì„œì¹˜ ìˆ˜í–‰ ì¤‘",
                "CONFIRM_STRATEGY": "ì „ëµ í™•ì¸ ëŒ€ê¸°",
                "GENERATING": "Introduction ì‘ì„± ì¤‘",
                "EVALUATING": "í’ˆì§ˆ í‰ê°€ ì¤‘",
                "EVALUATION_FAILED": "í‰ê°€ ì‹¤íŒ¨",
                "CONFIRM_EVOLUTION": "ê°œì„  í™•ì¸ ëŒ€ê¸°",
                "SELF_EVOLVING": "ìë™ ê°œì„  ì¤‘",
                "COMPLETE": "ì™„ë£Œ",
            }
            st.info(f"í˜„ì¬ ìƒíƒœ: **{state_labels.get(state, state)}**")
            iteration = st.session_state.pipeline_iteration
            if iteration > 0:
                st.write(f"Self-evolution ë°˜ë³µ: {iteration}/{MAX_EVOLUTION_ITERATIONS}")

        # History
        st.markdown("### ğŸ“‹ ìƒì„± ì´ë ¥")
        if st.session_state.history:
            for i, item in enumerate(st.session_state.history):
                if st.button(f"{i+1}. {item['topic'][:40]}...", key=f"history_{i}"):
                    st.session_state.current_topic = item["topic"]
                    st.session_state.generation_result = item["result"]
                    st.session_state.pipeline_state = "COMPLETE"
        else:
            st.info("ìƒì„± ì´ë ¥ì´ ì—†ìŠµë‹ˆë‹¤")

        st.markdown("---")
        st.markdown("### ğŸ“– ì •ë³´")
        st.markdown(
            "[GitHub](https://github.com/anthropics/claude-code) | "
            "[ë¬¸ì„œ](https://github.com/anthropics/claude-code/blob/main/README.md)"
        )

        return api_key, model, reference_style


# ------------------------------------------------------------------
# State renderers
# ------------------------------------------------------------------

def render_idle_state():
    """IDLE: Topic input and start"""
    st.markdown("## ì—°êµ¬ ì£¼ì œ ì…ë ¥")

    topic = st.text_area(
        "ì—°êµ¬ ì£¼ì œë¥¼ í•œ ì¤„ë¡œ ì…ë ¥í•˜ì„¸ìš”",
        placeholder="ì˜ˆ: Baseline resting-state EEGì™€ ë”¥ëŸ¬ë‹ì„ ì´ìš©í•œ ì¹˜ë£Œì €í•­ì„± ìš°ìš¸ì¦(TRD) í™˜ìì˜ í•­ìš°ìš¸ì œ ë°˜ì‘ ì˜ˆì¸¡",
        height=100,
        help="êµ¬ì²´ì ì¼ìˆ˜ë¡ ì¢‹ìŠµë‹ˆë‹¤:\n- ì§ˆí™˜ëª… (êµ¬ì²´ì  ì•„í˜• í¬í•¨)\n- ë°ì´í„° ìœ í˜• (ê¸°ë¡ ì¡°ê±´ ëª…ì‹œ)\n- ë¶„ì„ ë°©ë²•\n- ì˜ˆì¸¡/ë¶„ë¥˜ ëŒ€ìƒ\n\nëª¨í˜¸í•œ ë¶€ë¶„ì€ ë‹¤ìŒ ë‹¨ê³„ì—ì„œ í™•ì¸ì„ ìš”ì²­í•©ë‹ˆë‹¤."
    )

    col1, col2 = st.columns([1, 3])
    with col1:
        start_btn = st.button("ğŸš€ ì‹œì‘", key="start_btn")

    if start_btn:
        if not topic.strip():
            st.error("ì—°êµ¬ ì£¼ì œë¥¼ ì…ë ¥í•˜ì„¸ìš”")
            return
        api_key = st.session_state.api_key_stored
        provider = st.session_state.get("provider_stored", "openai")
        if not api_key:
            st.error("API í‚¤ë¥¼ ì…ë ¥í•˜ì„¸ìš”")
            return
        if provider == "openai" and not api_key.startswith("sk-"):
            st.error("ìœ íš¨í•œ OpenAI API í‚¤ë¥¼ ì…ë ¥í•˜ì„¸ìš”")
            return
        if provider == "azure_openai" and not st.session_state.get("azure_endpoint_stored"):
            st.error("Azure Endpointë¥¼ ì…ë ¥í•˜ì„¸ìš”")
            return

        st.session_state.current_topic = topic
        st.session_state.pipeline_state = "PARSING"
        st.rerun()


def render_parsing_state():
    """PARSING: Parse topic (auto-advance)"""
    st.markdown("## ì£¼ì œ ë¶„ì„ ì¤‘...")

    orch = get_orchestrator()
    if not orch:
        st.error("API í‚¤ë¥¼ í™•ì¸í•˜ì„¸ìš”")
        st.session_state.pipeline_state = "IDLE"
        return

    with st.spinner("ì£¼ì œë¥¼ ì‹¬ì¸µ ë¶„ì„í•˜ê³  ìˆìŠµë‹ˆë‹¤..."):
        try:
            topic_analysis = orch.parse_topic(st.session_state.current_topic)
            st.session_state.topic_analysis = topic_analysis
            st.session_state.search_queries = topic_analysis.get("search_queries", [])
            st.session_state.pipeline_state = "CONFIRM_QUERIES"
            st.rerun()
        except Exception as e:
            st.error(f"ì£¼ì œ ë¶„ì„ ì‹¤íŒ¨: {str(e)}")
            logger.error(f"Topic parsing error: {e}", exc_info=True)
            st.session_state.pipeline_state = "IDLE"


def render_confirm_queries_state():
    """CONFIRM_QUERIES: Checkpoint 1 - user reviews/edits queries"""
    st.markdown("## ê²€ìƒ‰ ì¿¼ë¦¬ í™•ì¸ (ì²´í¬í¬ì¸íŠ¸ 1)")

    topic_analysis = st.session_state.topic_analysis
    if not topic_analysis:
        st.session_state.pipeline_state = "IDLE"
        st.rerun()
        return

    # Display parsed topic info
    st.markdown("### íŒŒì‹±ëœ ì£¼ì œ ì •ë³´")
    col1, col2 = st.columns(2)
    with col1:
        st.write(f"**ì§ˆí™˜:** {topic_analysis.get('disease', 'N/A')}")
        st.write(f"**ë°ì´í„° ìœ í˜•:** {topic_analysis.get('data_type', 'N/A')}")
        st.write(f"**í•µì‹¬ ì´ˆì :** {topic_analysis.get('key_intervention_or_focus', 'N/A')}")
    with col2:
        st.write(f"**ë°©ë²•ë¡ :** {topic_analysis.get('methodology', 'N/A')}")
        st.write(f"**ì˜ˆì¸¡ ëŒ€ìƒ:** {topic_analysis.get('outcome', 'N/A')}")

    # Concept hierarchy
    concepts = topic_analysis.get("concept_hierarchy", [])
    if concepts:
        with st.expander("ê°œë… ê³„ì¸µêµ¬ì¡°", expanded=False):
            for concept in concepts[:8]:
                st.write(f"  - {concept}")

    # Topic ambiguity disambiguation
    ambiguities = topic_analysis.get("potential_ambiguities", [])
    if ambiguities:
        st.markdown("### ì£¼ì œ í•´ì„ í™•ì¸")
        st.caption("ì•„ë˜ í•­ëª©ì—ì„œ ì—°êµ¬ ì£¼ì œì˜ ì˜ë„ì— ë§ëŠ” í•´ì„ì„ ì„ íƒí•˜ê³ , í•„ìš”í•˜ë©´ ì¶”ê°€ ì„¤ëª…ì„ ì…ë ¥í•˜ì„¸ìš”.")
        resolutions = st.session_state.get("topic_resolutions", {})
        for i, amb in enumerate(ambiguities):
            aspect = amb.get("aspect", f"í•­ëª© {i+1}")
            question = amb.get("question", aspect)
            options = amb.get("options", [])
            default = amb.get("default", "")
            reasoning = amb.get("reasoning", "")

            if options:
                default_idx = 0
                if default in options:
                    default_idx = options.index(default)
                selected = st.radio(
                    f"**{question}**",
                    options,
                    index=default_idx,
                    key=f"ambiguity_{i}",
                    help=reasoning,
                )
                custom_note = st.text_input(
                    f"{aspect}ì— ëŒ€í•œ ì¶”ê°€ ì„¤ëª… (ì„ íƒì‚¬í•­)",
                    placeholder="ì—°êµ¬ ì˜ë„ì— ë§ê²Œ ì¶”ê°€ ë§¥ë½ì„ ììœ ë¡­ê²Œ ì…ë ¥í•˜ì„¸ìš”",
                    key=f"ambiguity_note_{i}",
                )
                resolutions[aspect] = {"choice": selected, "note": custom_note}
            else:
                resolutions[aspect] = {"choice": default, "note": ""}
        st.session_state.topic_resolutions = resolutions

    # Show banner if queries were regenerated from disambiguation
    if st.session_state.get("queries_regenerated"):
        st.info("ì£¼ì œ í•´ì„ ê²°ê³¼ê°€ ë°˜ì˜ëœ ìƒˆë¡œìš´ ì¿¼ë¦¬ì…ë‹ˆë‹¤. í™•ì¸ í›„ ë¦¬ì„œì¹˜ë¥¼ ì‹œì‘í•˜ì„¸ìš”.")

    # Editable search queries
    st.markdown("### ê²€ìƒ‰ ì¿¼ë¦¬ (í¸ì§‘ ê°€ëŠ¥)")
    st.caption("í•œ ì¤„ì— í•˜ë‚˜ì˜ ì¿¼ë¦¬ë¥¼ ì…ë ¥í•˜ì„¸ìš”. ì¿¼ë¦¬ë¥¼ ì¶”ê°€/ì‚­ì œ/ìˆ˜ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")

    queries_text = st.text_area(
        "ê²€ìƒ‰ ì¿¼ë¦¬",
        value="\n".join(st.session_state.search_queries),
        height=300,
        key="edit_queries"
    )

    # Additional feedback
    additional_feedback = st.text_area(
        "ì¶”ê°€ í”¼ë“œë°± (ì„ íƒì‚¬í•­)",
        placeholder="íŠ¹ë³„íˆ í¬í•¨í•˜ê³  ì‹¶ì€ ê²€ìƒ‰ ì „ëµì´ë‚˜ ê°•ì¡°ì ì´ ìˆìœ¼ë©´ ì‘ì„±í•˜ì„¸ìš”",
        height=80,
        key="query_feedback"
    )

    col1, col2 = st.columns(2)
    with col1:
        confirm_btn = st.button("í™•ì¸ & ë¦¬ì„œì¹˜ ì‹œì‘", key="confirm_queries_btn")
    with col2:
        back_btn = st.button("ì£¼ì œ ìˆ˜ì •ìœ¼ë¡œ ëŒì•„ê°€ê¸°", key="back_to_idle_btn")

    if confirm_btn:
        edited_queries = [q.strip() for q in queries_text.split("\n") if q.strip()]
        if not edited_queries:
            st.error("ìµœì†Œ í•˜ë‚˜ì˜ ê²€ìƒ‰ ì¿¼ë¦¬ê°€ í•„ìš”í•©ë‹ˆë‹¤")
            return

        orch = get_orchestrator()
        if not orch:
            st.error("API í‚¤ë¥¼ í™•ì¸í•˜ì„¸ìš”")
            return

        # Apply disambiguation resolutions to topic_analysis
        resolutions = st.session_state.get("topic_resolutions", {})
        if resolutions:
            st.session_state.topic_analysis["resolved_ambiguities"] = resolutions
            # Build resolution_text from new {choice, note} format
            resolution_parts = []
            for aspect, res in resolutions.items():
                if isinstance(res, dict):
                    part = f"{aspect}: {res.get('choice', '')}"
                    if res.get("note"):
                        part += f" ({res['note']})"
                else:
                    part = f"{aspect}: {res}"
                resolution_parts.append(part)
            resolution_text = "; ".join(resolution_parts)
            existing_ctx = st.session_state.topic_analysis.get("additional_context", "")
            st.session_state.topic_analysis["additional_context"] = (
                f"{existing_ctx}; {resolution_text}" if existing_ctx else resolution_text
            )

            # Two-pass flow: regenerate queries on first confirm if not already done
            if not st.session_state.get("queries_regenerated"):
                with st.spinner("ì£¼ì œ í•´ì„ ê²°ê³¼ë¥¼ ë°˜ì˜í•˜ì—¬ ì¿¼ë¦¬ë¥¼ ì¬ìƒì„±í•˜ê³  ìˆìŠµë‹ˆë‹¤..."):
                    new_queries = orch.regenerate_queries(
                        st.session_state.topic_analysis, resolutions, edited_queries
                    )
                st.session_state.search_queries = new_queries
                st.session_state.queries_regenerated = True
                st.rerun()
                return

        st.session_state.topic_analysis = orch.update_queries(
            st.session_state.topic_analysis, edited_queries
        )
        st.session_state.search_queries = edited_queries
        st.session_state.pipeline_state = "RESEARCHING"
        st.rerun()

    if back_btn:
        reset_pipeline()
        st.rerun()


def render_researching_state():
    """RESEARCHING: Run research pipeline with per-query progress"""
    st.markdown("## ë¦¬ì„œì¹˜ ìˆ˜í–‰ ì¤‘...")

    orch = get_orchestrator()
    if not orch:
        st.error("API í‚¤ë¥¼ í™•ì¸í•˜ì„¸ìš”")
        st.session_state.pipeline_state = "IDLE"
        return

    status_container = st.status("PubMed ë¦¬ì„œì¹˜ ì§„í–‰ ì¤‘...", expanded=True)

    try:
        with status_container:
            from utils.pubmed_utils import has_valid_abstract

            topic_analysis = st.session_state.topic_analysis
            search_queries = topic_analysis.get("search_queries", [])
            st.write(f"**{len(search_queries)}ê°œ ì¿¼ë¦¬ë¡œ ë…¼ë¬¸ ìˆ˜ì§‘ ì‹œì‘...**")

            # --- Paper collection with per-query feedback ---
            researcher = orch.deep_researcher
            all_papers = {}

            for i, query in enumerate(search_queries, 1):
                try:
                    papers = researcher.pubmed_client.search_and_fetch(
                        query, f"query_{i}", max_results=30
                    )
                    added = 0
                    retracted = 0
                    for paper in papers:
                        pmid = paper.get("pmid")
                        if pmid and pmid not in all_papers and has_valid_abstract(paper):
                            if paper.get("is_retracted", False):
                                retracted += 1
                                continue
                            all_papers[pmid] = paper
                            added += 1
                    retracted_note = f" (retracted {retracted}í¸ ì œì™¸)" if retracted else ""
                    st.write(f"  Query {i}/{len(search_queries)}: +{added}í¸{retracted_note} (ëˆ„ì  {len(all_papers)}í¸) â€” `{query[:55]}`")
                except Exception as e:
                    st.write(f"  Query {i}/{len(search_queries)}: ì‹¤íŒ¨ â€” {str(e)[:50]}")

            # High-impact journal search
            disease = topic_analysis.get("disease", "")
            if disease:
                try:
                    hi_papers = researcher.pubmed_client.search_and_fetch(
                        f"({disease}) AND (Nature[Journal] OR NEJM[Journal] OR Lancet[Journal] OR JAMA[Journal])",
                        "high_impact", max_results=40
                    )
                    added = 0
                    for paper in hi_papers:
                        pmid = paper.get("pmid")
                        if pmid and pmid not in all_papers and has_valid_abstract(paper):
                            if paper.get("is_retracted", False):
                                continue
                            all_papers[pmid] = paper
                            added += 1
                    st.write(f"  High-impact journals: +{added}í¸ (ëˆ„ì  {len(all_papers)}í¸)")
                except Exception as e:
                    st.write(f"  High-impact journals: ì‹¤íŒ¨ â€” {str(e)[:50]}")

            paper_pool = list(all_papers.values())
            st.session_state.paper_pool = paper_pool
            st.write(f"**ì´ {len(paper_pool)}ê°œ ë…¼ë¬¸ ìˆ˜ì§‘ ì™„ë£Œ**")

        # --- 0 papers guard ---
        if not paper_pool:
            st.error(
                "PubMedì—ì„œ ë…¼ë¬¸ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ê°€ëŠ¥í•œ ì›ì¸:\n"
                "- ê²€ìƒ‰ ì¿¼ë¦¬ê°€ ë„ˆë¬´ êµ¬ì²´ì ì´ê±°ë‚˜ PubMed ë¬¸ë²•ì— ë§ì§€ ì•ŠìŒ\n"
                "- PubMed API ì¼ì‹œ ì¥ì•  ë˜ëŠ” ë„¤íŠ¸ì›Œí¬ ë¬¸ì œ\n\n"
                "ì¿¼ë¦¬ë¥¼ ìˆ˜ì •í•œ ë’¤ ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì„¸ìš”."
            )
            st.session_state.pipeline_state = "CONFIRM_QUERIES"
            return

        # --- Landscape analysis ---
        with st.status("ë¬¸í—Œ ë¶„ì„ ë° ì „ëµ ìˆ˜ë¦½ ì¤‘...", expanded=True) as status2:
            st.write("**ë¬¸í—Œ ê²½ê´€ ë¶„ì„ ì¤‘...**")
            landscape = orch.intro_generator.step_analyze_landscape(
                paper_pool, st.session_state.current_topic
            )
            st.session_state.landscape = landscape
            st.write(f"  í•µì‹¬ ë°œê²¬ì‚¬í•­ {len(landscape.get('key_findings', []))}ê°œ, "
                      f"ë¯¸ì¶©ì¡± ë¶„ì•¼ {len(landscape.get('knowledge_gaps', []))}ê°œ ì‹ë³„")

            # --- Reference pool selection ---
            st.write("**ìµœì  ë…¼ë¬¸ í’€ ì„ ë³„ ì¤‘...**")
            reference_pool = orch.intro_generator.step_select_references(paper_pool, landscape)
            st.session_state.reference_pool = reference_pool
            st.write(f"  {len(reference_pool)}í¸ ì„ ë³„ ì™„ë£Œ")

            # --- Writing strategy ---
            st.write("**Writing strategy ìƒì„± ì¤‘...**")
            strategy = orch.generate_writing_strategy(
                st.session_state.topic_analysis, reference_pool, landscape
            )
            st.session_state.writing_strategy = strategy
            st.write("**ë¦¬ì„œì¹˜ ì™„ë£Œ!**")

        st.session_state.pipeline_state = "CONFIRM_STRATEGY"
        st.rerun()

    except Exception as e:
        st.error(f"ë¦¬ì„œì¹˜ ì‹¤íŒ¨: {str(e)}")
        logger.error(f"Research error: {e}", exc_info=True)
        st.session_state.pipeline_state = "CONFIRM_QUERIES"


def render_confirm_strategy_state():
    """CONFIRM_STRATEGY: Checkpoint 2 - user reviews strategy"""
    st.markdown("## Writing Strategy í™•ì¸ (ì²´í¬í¬ì¸íŠ¸ 2)")

    # Research summary
    st.markdown("### ë¦¬ì„œì¹˜ ìš”ì•½")
    col1, col2, col3 = st.columns(3)
    with col1:
        st.metric("ìˆ˜ì§‘ëœ ë…¼ë¬¸", len(st.session_state.paper_pool))
    with col2:
        st.metric("ì„ ë³„ëœ ë…¼ë¬¸", len(st.session_state.reference_pool))
    with col3:
        landscape = st.session_state.landscape
        findings_count = len(landscape.get("key_findings", []))
        st.metric("í•µì‹¬ ë°œê²¬ì‚¬í•­", findings_count)

    # Writing strategy display
    strategy = st.session_state.writing_strategy
    paragraphs = strategy.get("paragraphs", [])
    ref_pool = st.session_state.reference_pool

    if paragraphs:
        st.markdown("### Paragraphë³„ ê³„íš")
        for para in paragraphs:
            num = para.get("paragraph_number", "?")
            topic = para.get("topic", "")
            key_points = para.get("key_points", [])
            refs = para.get("supporting_papers", [])
            transition = para.get("transition_to_next", "")

            with st.expander(f"Paragraph {num}: {topic}", expanded=True):
                st.markdown("**Key Points:**")
                for pt in key_points:
                    st.write(f"  - {pt}")

                # Show reference details
                if refs and ref_pool:
                    st.markdown("**Supporting References:**")
                    for ref_num in refs:
                        idx = ref_num - 1  # 1-indexed to 0-indexed
                        if 0 <= idx < len(ref_pool):
                            paper = ref_pool[idx]
                            authors = paper.get("authors", [])
                            first_author = authors[0] if authors else "Unknown"
                            et_al = " et al." if len(authors) > 1 else ""
                            title = paper.get("title", "")[:80]
                            journal = paper.get("journal_iso", paper.get("journal", ""))
                            year = paper.get("pub_year", "")
                            st.write(f"  [{ref_num}] {first_author}{et_al} â€” {title}... *{journal}* ({year})")
                        else:
                            st.write(f"  [{ref_num}] (ë²”ìœ„ ë°–)")
                elif refs:
                    st.write(f"**Supporting references:** [{', '.join(str(r) for r in refs)}]")

                if transition:
                    st.write(f"**Transition:** {transition}")

    narrative_arc = strategy.get("narrative_arc", "")
    if narrative_arc:
        st.markdown("### Narrative Arc")
        st.info(narrative_arc)

    # Landscape details
    with st.expander("ë¬¸í—Œ ê²½ê´€ ë¶„ì„ ìƒì„¸", expanded=False):
        _display_landscape(st.session_state.landscape)

    # Feedback
    st.markdown("---")
    feedback = st.text_area(
        "ì „ëµì— ëŒ€í•œ í”¼ë“œë°± (ì„ íƒì‚¬í•­)",
        placeholder="ì˜ˆ: 'ë°©ë²•ë¡  íŒŒíŠ¸ë¥¼ ë” ìì„¸í•˜ê²Œ' ë˜ëŠ” 'íŠ¹ì • ë…¼ë¬¸ì„ ë” ê°•ì¡°í•´ì¤˜'",
        height=80,
        key="strategy_feedback"
    )

    auto_evolve = st.checkbox(
        "ìë™ ë°˜ë³µ ê°œì„  (ìˆ˜ë™ í™•ì¸ ì—†ì´)",
        value=st.session_state.get("evolution_auto_mode", False),
        help="í™œì„±í™”í•˜ë©´ í’ˆì§ˆ í‰ê°€ í›„ ê¸°ì¤€ ë¯¸ë‹¬ ì‹œ ìë™ìœ¼ë¡œ ê°œì„ ì„ ë°˜ë³µí•©ë‹ˆë‹¤",
        key="auto_evolve_toggle"
    )
    st.session_state.evolution_auto_mode = auto_evolve

    col1, col2, col3 = st.columns(3)
    with col1:
        generate_btn = st.button("Introduction ì‘ì„±", key="generate_intro_btn")
    with col2:
        add_research_btn = st.button("ì¶”ê°€ ë¦¬ì„œì¹˜ í›„ ì‘ì„±", key="add_research_btn")
    with col3:
        back_btn = st.button("ì¿¼ë¦¬ ìˆ˜ì •ìœ¼ë¡œ ëŒì•„ê°€ê¸°", key="back_to_queries_btn")

    if generate_btn:
        st.session_state.show_additional_research = False
        st.session_state.pipeline_state = "GENERATING"
        st.rerun()

    if add_research_btn:
        st.session_state.show_additional_research = True
        st.rerun()

    # Persistent additional research UI (survives Streamlit reruns)
    if st.session_state.show_additional_research:
        st.markdown("### ì¶”ê°€ ë¦¬ì„œì¹˜")
        input_mode = st.radio(
            "ì…ë ¥ ë°©ì‹",
            ["ìì—°ì–´ í”¼ë“œë°± (AIê°€ ì¿¼ë¦¬ ìƒì„±)", "ì§ì ‘ PubMed ì¿¼ë¦¬ ì…ë ¥"],
            key="extra_research_mode"
        )

        if input_mode == "ìì—°ì–´ í”¼ë“œë°± (AIê°€ ì¿¼ë¦¬ ìƒì„±)":
            nl_feedback = st.text_area(
                "ì–´ë–¤ ë¶€ë¶„ì´ ë¶€ì¡±í•œì§€ ìì—°ì–´ë¡œ ì„¤ëª…í•˜ì„¸ìš”",
                placeholder="ì˜ˆ: treatment-resistant MDD ê´€ë ¨ ë…¼ë¬¸ì´ ë¶€ì¡±í•´ìš” / ë°©ë²•ë¡  íŒŒíŠ¸ì— EEG ì „ì²˜ë¦¬ ê´€ë ¨ ë‚´ìš©ì´ í•„ìš”í•´ìš”",
                height=100,
                key="nl_feedback_input"
            )
            if st.button("AI ì¿¼ë¦¬ ìƒì„± & ë¦¬ì„œì¹˜ ì‹¤í–‰", key="run_nl_research_btn"):
                if not nl_feedback.strip():
                    st.warning("í”¼ë“œë°±ì„ ì…ë ¥í•˜ì„¸ìš”")
                else:
                    orch = get_orchestrator()
                    if not orch:
                        st.error("API í‚¤ë¥¼ í™•ì¸í•˜ì„¸ìš”")
                    else:
                        with st.spinner("í”¼ë“œë°±ì„ ë¶„ì„í•˜ì—¬ ê²€ìƒ‰ ì¿¼ë¦¬ë¥¼ ìƒì„±í•˜ê³  ìˆìŠµë‹ˆë‹¤..."):
                            result = orch.generate_queries_from_feedback(
                                user_feedback=nl_feedback,
                                writing_strategy=st.session_state.writing_strategy,
                                topic_analysis=st.session_state.topic_analysis,
                                landscape=st.session_state.landscape
                            )
                        interpretation = result.get("interpretation", "")
                        queries = result.get("queries", [])
                        if interpretation:
                            st.info(f"**AI í•´ì„:** {interpretation}")
                        if queries:
                            st.write(f"**ìƒì„±ëœ ì¿¼ë¦¬ ({len(queries)}ê°œ):**")
                            for i, q in enumerate(queries, 1):
                                st.write(f"  {i}. `{q}`")
                            _run_supplementary_research(queries)
                        else:
                            st.warning("ì¿¼ë¦¬ë¥¼ ìƒì„±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ì§ì ‘ ì…ë ¥ì„ ì‹œë„í•´ ì£¼ì„¸ìš”.")
        else:
            extra_queries = st.text_area(
                "ì¶”ê°€ ì¿¼ë¦¬ ì…ë ¥ (í•œ ì¤„ì— í•˜ë‚˜)",
                placeholder="ì¶”ê°€ë¡œ ê²€ìƒ‰í•˜ê³  ì‹¶ì€ ì¿¼ë¦¬ë¥¼ ì…ë ¥í•˜ì„¸ìš”",
                height=120,
                key="extra_queries_input"
            )
            if st.button("ì¶”ê°€ ë¦¬ì„œì¹˜ ì‹¤í–‰", key="run_extra_research_btn"):
                queries = [q.strip() for q in extra_queries.split("\n") if q.strip()]
                if not queries:
                    st.warning("ì¶”ê°€ ì¿¼ë¦¬ë¥¼ ì…ë ¥í•˜ì„¸ìš”")
                else:
                    _run_supplementary_research(queries)

        if st.button("ì·¨ì†Œ", key="cancel_extra_research_btn"):
            st.session_state.show_additional_research = False
            st.rerun()

    if back_btn:
        st.session_state.show_additional_research = False
        st.session_state.pipeline_state = "CONFIRM_QUERIES"
        st.rerun()


def _run_supplementary_research(queries: list):
    """Run supplementary research and update state"""
    orch = get_orchestrator()
    if not orch:
        st.error("API í‚¤ë¥¼ í™•ì¸í•˜ì„¸ìš”")
        return

    with st.spinner("ì¶”ê°€ ë¦¬ì„œì¹˜ ìˆ˜í–‰ ì¤‘..."):
        try:
            expanded_pool, updated_landscape, new_ref_pool = orch.run_supplementary_research(
                additional_queries=queries,
                paper_pool=st.session_state.paper_pool,
                landscape=st.session_state.landscape,
                research_topic=st.session_state.current_topic,
                topic_analysis=st.session_state.topic_analysis,
            )
            old_count = len(st.session_state.paper_pool)
            st.session_state.paper_pool = expanded_pool
            st.session_state.landscape = updated_landscape
            st.session_state.reference_pool = new_ref_pool

            # Re-generate writing strategy
            strategy = orch.generate_writing_strategy(
                st.session_state.topic_analysis, new_ref_pool, updated_landscape
            )
            st.session_state.writing_strategy = strategy

            new_count = len(expanded_pool) - old_count
            st.session_state.show_additional_research = False
            st.success(f"ì¶”ê°€ ë¦¬ì„œì¹˜ ì™„ë£Œ! {new_count}í¸ ìƒˆ ë…¼ë¬¸ ë°œê²¬. Reference pool: {len(new_ref_pool)}í¸")
            st.rerun()
        except Exception as e:
            st.error(f"ì¶”ê°€ ë¦¬ì„œì¹˜ ì‹¤íŒ¨: {str(e)}")
            logger.error(f"Supplementary research error: {e}", exc_info=True)


def render_generating_state():
    """GENERATING: Generate introduction (auto-advance)"""
    st.markdown("## Introduction ì‘ì„± ì¤‘...")

    orch = get_orchestrator()
    if not orch:
        st.error("API í‚¤ë¥¼ í™•ì¸í•˜ì„¸ìš”")
        st.session_state.pipeline_state = "IDLE"
        return

    with st.spinner("Introductionì„ ì‘ì„±í•˜ê³  ìˆìŠµë‹ˆë‹¤..."):
        try:
            introduction = orch.generate_introduction(
                st.session_state.topic_analysis,
                st.session_state.reference_pool,
                st.session_state.landscape,
                writing_strategy=st.session_state.get("writing_strategy"),
            )
            # Validate citation range and renumber
            introduction = PipelineOrchestrator.validate_citation_range(
                introduction, len(st.session_state.reference_pool)
            )
            introduction, st.session_state.reference_pool = \
                PipelineOrchestrator.renumber_citations(
                    introduction, st.session_state.reference_pool
                )
            st.session_state.introduction_text = introduction
            st.session_state.pipeline_state = "EVALUATING"
            st.rerun()
        except Exception as e:
            st.error(f"Introduction ì‘ì„± ì‹¤íŒ¨: {str(e)}")
            logger.error(f"Generation error: {e}", exc_info=True)
            st.session_state.pipeline_state = "CONFIRM_STRATEGY"


def render_evaluating_state():
    """EVALUATING: Run 10-criterion evaluation (auto-advance)"""
    st.markdown("## í’ˆì§ˆ í‰ê°€ ì¤‘...")

    orch = get_orchestrator()
    if not orch:
        st.error("API í‚¤ë¥¼ í™•ì¸í•˜ì„¸ìš”")
        st.session_state.pipeline_state = "IDLE"
        return

    with st.spinner("10ê°œ ê¸°ì¤€ìœ¼ë¡œ í’ˆì§ˆì„ í‰ê°€í•˜ê³  ìˆìŠµë‹ˆë‹¤..."):
        try:
            evaluation = orch.evaluate_introduction(
                st.session_state.introduction_text,
                st.session_state.reference_pool,
                st.session_state.topic_analysis,
                st.session_state.landscape
            )
            st.session_state.evaluation_result = evaluation

            # Auto fact-check
            try:
                fact_result = orch.run_fact_check(
                    st.session_state.introduction_text,
                    st.session_state.reference_pool
                )
                st.session_state.fact_check_result = fact_result

                # Derive fact-checker score and use the LOWER of evaluator vs fact-checker
                # (avoids double-counting: both assess the same issues from different angles)
                claim_mapping = fact_result.get("claim_mapping", {})
                current_fa = evaluation.get("scores", {}).get("factual_accuracy", 10)
                unsupported = [
                    c for c in claim_mapping.get("claim_mappings", [])
                    if not c.get("is_supported", True)
                ]
                major_mismatches = sum(
                    1 for nm in claim_mapping.get("numerical_mismatches", [])
                    if nm.get("severity") == "major"
                )
                num_mismatches = len(claim_mapping.get("numerical_mismatches", []))
                # Compute fact-checker implied score using ratio-based thresholds
                total_claims = len(claim_mapping.get("claim_mappings", []))
                unsupported_ratio = len(unsupported) / max(total_claims, 1)

                if major_mismatches >= 3 or unsupported_ratio >= 0.30:
                    fc_implied = 5      # 30%+ unsupported = severe
                elif major_mismatches >= 1 or unsupported_ratio >= 0.15 or len(unsupported) >= 5:
                    fc_implied = 7      # 15-30% unsupported = significant
                elif len(unsupported) >= 1 or num_mismatches >= 1:
                    fc_implied = 8      # Minor issues
                elif num_mismatches == 0 and len(unsupported) == 0:
                    fc_implied = 9      # No issues
                else:
                    fc_implied = current_fa
                # Use the LOWER of the two (no additive double-penalty)
                evaluation["scores"]["factual_accuracy"] = min(current_fa, fc_implied)

                # Recalculate overall score
                scores = evaluation.get("scores", {})
                if scores:
                    evaluation["overall_score"] = round(
                        sum(scores.values()) / len(scores), 1
                    )
            except Exception as fc_err:
                logger.warning(f"Auto fact-check failed (non-blocking): {fc_err}")

            # Record iteration history
            iteration = st.session_state.pipeline_iteration
            st.session_state.iteration_history.append({
                "iteration": iteration,
                "label": "Draft" if iteration == 0 else f"Rev{iteration}",
                "introduction": st.session_state.introduction_text,
                "evaluation": evaluation,
            })

            # Check if self-evolution is needed
            is_first_draft = st.session_state.pipeline_iteration == 0
            overall = evaluation.get("overall_score", 0)
            fa = evaluation.get("scores", {}).get("factual_accuracy", 0)
            auto_mode = st.session_state.get("evolution_auto_mode", False)

            if is_first_draft or (
                orch.needs_self_evolution(evaluation)
                and st.session_state.pipeline_iteration < MAX_EVOLUTION_ITERATIONS
            ):
                if auto_mode:
                    logger.info(
                        f"â†’ SELF_EVOLVING (overall={overall}, fa={fa}, "
                        f"first_draft={is_first_draft}, auto={auto_mode})"
                    )
                    st.session_state.pipeline_state = "SELF_EVOLVING"
                else:
                    logger.info(
                        f"â†’ CONFIRM_EVOLUTION (overall={overall}, fa={fa}, "
                        f"first_draft={is_first_draft}, auto={auto_mode})"
                    )
                    st.session_state.pipeline_state = "CONFIRM_EVOLUTION"
            else:
                logger.info(
                    f"â†’ COMPLETE (overall={overall}, fa={fa}, "
                    f"iteration={st.session_state.pipeline_iteration})"
                )
                st.session_state.pipeline_state = "COMPLETE"

            st.rerun()

        except Exception as e:
            logger.error(f"Evaluation error: {e}", exc_info=True)
            st.session_state.evaluation_error = str(e)
            st.session_state.pipeline_state = "EVALUATION_FAILED"
            st.rerun()


def render_evaluation_failed_state():
    """EVALUATION_FAILED: Show evaluation error and offer retry or skip"""
    st.markdown("## í’ˆì§ˆ í‰ê°€ ì‹¤íŒ¨")

    error_msg = st.session_state.get("evaluation_error", "ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜")
    st.error(f"í’ˆì§ˆ í‰ê°€ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {error_msg}")

    col1, col2 = st.columns(2)
    with col1:
        if st.button("í‰ê°€ ì¬ì‹œë„", key="retry_evaluation_btn"):
            st.session_state.evaluation_error = None
            st.session_state.pipeline_state = "EVALUATING"
            st.rerun()
    with col2:
        if st.button("í‰ê°€ ì—†ì´ ì™„ë£Œ", key="skip_evaluation_btn"):
            st.session_state.evaluation_error = None
            st.session_state.pipeline_state = "COMPLETE"
            st.rerun()


def render_confirm_evolution_state():
    """CONFIRM_EVOLUTION: User reviews evaluation and decides whether to evolve"""
    iteration = st.session_state.pipeline_iteration
    next_iteration = iteration + 1
    st.markdown(f"## ìë™ ê°œì„  í™•ì¸ (Iteration {next_iteration}/{MAX_EVOLUTION_ITERATIONS})")

    evaluation = st.session_state.evaluation_result
    if not evaluation:
        st.session_state.pipeline_state = "COMPLETE"
        st.rerun()
        return

    orch = get_orchestrator()
    if not orch:
        st.error("API í‚¤ë¥¼ í™•ì¸í•˜ì„¸ìš”")
        st.session_state.pipeline_state = "COMPLETE"
        st.rerun()
        return

    # --- 1. Score summary metrics ---
    overall_score = evaluation.get("overall_score", 0)
    factual_acc = evaluation.get("scores", {}).get("factual_accuracy", 0)
    col1, col2, col3 = st.columns(3)
    with col1:
        st.metric("ì¢…í•© ì ìˆ˜", f"{overall_score}/10")
    with col2:
        st.metric("Factual Accuracy", f"{factual_acc}/10")
    with col3:
        label = "Draft" if iteration == 0 else f"Draft + {iteration} Rev"
        st.metric("í˜„ì¬ Iteration", label)

    # --- 2. Detailed score grid (reuse existing helper) ---
    display_self_evaluation_results(evaluation)

    # --- 3. Key issues: low-scoring criteria feedback ---
    scores = evaluation.get("scores", {})
    feedback = evaluation.get("feedback", {})
    improvements = evaluation.get("improvements", [])
    weak = [(k, v, feedback.get(k, "")) for k, v in scores.items()
            if isinstance(v, (int, float)) and v < 8]

    # --- Introduction preview ---
    st.markdown("### í˜„ì¬ Introduction")
    with st.expander("Introduction ë¯¸ë¦¬ë³´ê¸°", expanded=False):
        st.markdown(st.session_state.introduction_text)

    if weak:
        st.markdown("### ì£¼ìš” ì´ìŠˆ")
        for criterion, score, fb in weak:
            with st.expander(f"**{criterion.replace('_', ' ').title()}** (ì ìˆ˜: {score}/10)", expanded=True):
                st.warning(fb)
                for imp in improvements:
                    if imp.get("criterion") == criterion and imp.get("improvement"):
                        st.info(f"**ê°œì„  ì œì•ˆ:** {imp['improvement']}")
                        break

        # 4c: Revision direction summary from weak criteria improvements
        revision_directions = []
        for criterion, score, fb in weak:
            criterion_label = criterion.replace('_', ' ').title()
            for imp in improvements:
                if imp.get("criterion") == criterion and imp.get("improvement"):
                    imp_text = imp["improvement"]
                    imp_lines = imp_text.strip().split("\n")
                    header = f"- **{criterion_label}**: {imp_lines[0]}"
                    if len(imp_lines) > 1:
                        continuation = "\n".join(f"  {line}" for line in imp_lines[1:])
                        revision_directions.append(f"{header}\n{continuation}")
                    else:
                        revision_directions.append(header)
                    break
        if revision_directions:
            st.markdown("**ìˆ˜ì • ë°©í–¥ ìš”ì•½:**\n\n" + "\n\n".join(revision_directions))

    # --- 4. Extract unsupported claims + completeness gaps (parallel, cached) ---
    need_claims = not st.session_state.evolution_claims
    comp_score = scores.get("completeness", 10)
    need_gaps = not st.session_state.get("evolution_completeness_gaps") and comp_score < 8

    if need_claims and need_gaps:
        # Both uncached â€” run in parallel
        with st.spinner("ë¯¸ì§€ì§€ í´ë ˆì„ + Completeness gap ë¶„ì„ ì¤‘..."):
            with ThreadPoolExecutor(max_workers=2) as executor:
                f_claims = executor.submit(
                    orch.extract_unsupported_claims,
                    evaluation, st.session_state.introduction_text
                )
                f_gaps = executor.submit(
                    orch.extract_completeness_gaps,
                    evaluation, st.session_state.introduction_text,
                    st.session_state.landscape
                )
                st.session_state.evolution_claims = f_claims.result()
                st.session_state.evolution_completeness_gaps = f_gaps.result()
    elif need_claims:
        with st.spinner("ë¯¸ì§€ì§€ í´ë ˆì„ ë¶„ì„ ì¤‘..."):
            st.session_state.evolution_claims = orch.extract_unsupported_claims(
                evaluation, st.session_state.introduction_text
            )
    elif need_gaps:
        with st.spinner("Completeness gap ë¶„ì„ ì¤‘..."):
            st.session_state.evolution_completeness_gaps = orch.extract_completeness_gaps(
                evaluation, st.session_state.introduction_text,
                st.session_state.landscape
            )

    claims = st.session_state.evolution_claims
    if claims:
        st.markdown(f"### ë¯¸ì§€ì§€ í´ë ˆì„ ({len(claims)}ê°œ)")
        for i, claim_info in enumerate(claims, 1):
            claim_text = claim_info.get("claim", "")
            issue = claim_info.get("issue", "")
            needed = claim_info.get("needed_evidence", "")
            with st.expander(f"Claim {i}: {claim_text[:80]}{'...' if len(claim_text) > 80 else ''}", expanded=False):
                st.write(f"**ì›ë¬¸:** {claim_text}")
                if issue:
                    st.write(f"**ì´ìŠˆ:** {issue}")
                if needed:
                    st.write(f"**í•„ìš”í•œ ê·¼ê±°:** {needed}")
    else:
        st.info("ë¯¸ì§€ì§€ í´ë ˆì„ì´ ë°œê²¬ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

    comp_gaps = st.session_state.get("evolution_completeness_gaps", [])
    if comp_gaps:
        st.markdown(f"### Completeness Gaps ({len(comp_gaps)}ê°œ)")
        for i, gap_info in enumerate(comp_gaps, 1):
            source = gap_info.get("source", "")
            item_num = gap_info.get("item_number", "?")
            item_text = gap_info.get("item_text", "")
            gap_desc = gap_info.get("gap_description", "")
            needed = gap_info.get("needed_evidence", "")
            label = f"{'Key Finding' if source == 'key_finding' else 'Knowledge Gap'} #{item_num}"
            with st.expander(f"Gap {i}: {label} â€” {item_text[:60]}{'...' if len(item_text) > 60 else ''}", expanded=False):
                st.write(f"**ëˆ„ë½ í•­ëª©:** {item_text}")
                if gap_desc:
                    st.write(f"**ì„¤ëª…:** {gap_desc}")
                if needed:
                    st.write(f"**í•„ìš”í•œ ê·¼ê±°:** {needed}")

    # --- 5. Generate supplementary query plan (cache) ---
    # Merge claims + completeness gaps for query generation
    all_items_for_queries = list(claims) + orch._completeness_gaps_as_claims(comp_gaps)
    if all_items_for_queries and not st.session_state.evolution_queries:
        with st.spinner("ë³´ì¶© ê²€ìƒ‰ ì¿¼ë¦¬ ìƒì„± ì¤‘..."):
            queries = orch.generate_supplementary_queries(
                all_items_for_queries, st.session_state.topic_analysis
            )
            st.session_state.evolution_queries = queries

    queries = st.session_state.evolution_queries
    if queries:
        st.markdown(f"### ë³´ì¶© ê²€ìƒ‰ ì¿¼ë¦¬ ê³„íš ({len(queries)}ê°œ)")
        for i, q in enumerate(queries, 1):
            if isinstance(q, dict):
                query_str = q.get("query", "")
                rationale = q.get("rationale", "")
                with st.expander(f"{i}. `{query_str}`", expanded=False):
                    if rationale:
                        st.write(f"**ê·¼ê±°:** {rationale}")
            else:
                st.write(f"  {i}. `{q}`")

    # --- 6. User feedback input ---
    st.markdown("### ì‚¬ìš©ì í”¼ë“œë°± (ì„ íƒì‚¬í•­)")
    user_fb = st.text_area(
        "ì¶”ê°€ í”¼ë“œë°±",
        value=st.session_state.get("user_evolution_feedback", ""),
        placeholder="ì˜ˆ: 'ë‘ ë²ˆì§¸ ë¬¸ë‹¨ì˜ ë”¥ëŸ¬ë‹ ë¶€ë¶„ì„ ë” êµ¬ì²´ì ìœ¼ë¡œ ì‘ì„±í•´ì¤˜' ë˜ëŠ” 'íŠ¹ì • ë…¼ë¬¸ì„ ë” ê°•ì¡°í•´ì¤˜'",
        height=80,
        key="evolution_feedback_input"
    )

    # --- 7. Action buttons ---
    st.markdown("---")
    col1, col2, col3 = st.columns(3)
    with col1:
        proceed_btn = st.button("ìë™ ê°œì„  ì§„í–‰", key="evolution_proceed_btn")
    with col2:
        auto_btn = st.button("ì´í›„ ìë™ (ì „ì²´)", key="evolution_auto_btn",
                              help="ì´í›„ iterationì€ í™•ì¸ ì—†ì´ ìë™ ì§„í–‰í•©ë‹ˆë‹¤")
    with col3:
        accept_btn = st.button("í˜„ì¬ ë²„ì „ ìˆ˜ë½", key="evolution_accept_btn")

    if proceed_btn:
        st.session_state.user_evolution_feedback = user_fb
        st.session_state.pipeline_state = "SELF_EVOLVING"
        st.rerun()

    if auto_btn:
        st.session_state.user_evolution_feedback = user_fb
        st.session_state.evolution_auto_mode = True
        st.session_state.pipeline_state = "SELF_EVOLVING"
        st.rerun()

    if accept_btn:
        st.session_state.pipeline_state = "COMPLETE"
        st.rerun()


def render_self_evolving_state():
    """SELF_EVOLVING: Auto-improve introduction"""
    iteration = st.session_state.pipeline_iteration + 1
    st.markdown(f"## ìë™ ê°œì„  ì¤‘ (Iteration {iteration}/{MAX_EVOLUTION_ITERATIONS})")

    orch = get_orchestrator()
    if not orch:
        st.error("API í‚¤ë¥¼ í™•ì¸í•˜ì„¸ìš”")
        st.session_state.pipeline_state = "COMPLETE"
        st.rerun()
        return

    status_container = st.status("Self-evolution ì§„í–‰ ì¤‘...", expanded=True)

    # Retrieve user feedback
    user_feedback = st.session_state.get("user_evolution_feedback", "")

    try:
        with status_container:
            # Step 1 + 1b: Extract unsupported claims and completeness gaps
            cached_claims = st.session_state.get("evolution_claims", [])
            cached_comp_gaps = st.session_state.get("evolution_completeness_gaps", [])
            need_claims = not cached_claims
            need_gaps = not cached_comp_gaps

            if need_claims and need_gaps:
                # Both uncached â€” run in parallel
                st.write("ë¯¸ì§€ì§€ í´ë ˆì„ + Completeness gap ì¶”ì¶œ ì¤‘ (ë³‘ë ¬)...")
                with ThreadPoolExecutor(max_workers=2) as executor:
                    f_claims = executor.submit(
                        orch.extract_unsupported_claims,
                        st.session_state.evaluation_result,
                        st.session_state.introduction_text
                    )
                    f_gaps = executor.submit(
                        orch.extract_completeness_gaps,
                        st.session_state.evaluation_result,
                        st.session_state.introduction_text,
                        st.session_state.landscape,
                    )
                    claims = f_claims.result()
                    comp_gaps = f_gaps.result()
                st.write(f"  {len(claims)}ê°œ í´ë ˆì„, {len(comp_gaps)}ê°œ gap ë°œê²¬")
            else:
                if cached_claims:
                    claims = cached_claims
                    st.write(f"ìºì‹œëœ ë¯¸ì§€ì§€ í´ë ˆì„ ì‚¬ìš©: {len(claims)}ê°œ")
                else:
                    st.write("ë¯¸ì§€ì§€ í´ë ˆì„ ì¶”ì¶œ ì¤‘...")
                    claims = orch.extract_unsupported_claims(
                        st.session_state.evaluation_result,
                        st.session_state.introduction_text
                    )
                    st.write(f"  {len(claims)}ê°œ ë¯¸ì§€ì§€ í´ë ˆì„ ë°œê²¬")

                if cached_comp_gaps:
                    comp_gaps = cached_comp_gaps
                    st.write(f"ìºì‹œëœ completeness gap ì‚¬ìš©: {len(comp_gaps)}ê°œ")
                else:
                    st.write("Completeness gap ì¶”ì¶œ ì¤‘...")
                    comp_gaps = orch.extract_completeness_gaps(
                        st.session_state.evaluation_result,
                        st.session_state.introduction_text,
                        st.session_state.landscape,
                    )
                    if comp_gaps:
                        st.write(f"  {len(comp_gaps)}ê°œ completeness gap ë°œê²¬")
                    else:
                        st.write("  Completeness gap ì—†ìŒ (ì ìˆ˜ ì¶©ë¶„)")

            if not claims and not comp_gaps:
                # No specific claims/gaps, but scores still below threshold.
                # Fallback: regenerate using evaluation feedback only (skip supplementary search).
                st.write("êµ¬ì²´ì  í´ë ˆì„/gap ì—†ìŒ â€” í‰ê°€ í”¼ë“œë°± ê¸°ë°˜ìœ¼ë¡œ ì¬ì‘ì„±í•©ë‹ˆë‹¤.")
                if user_feedback:
                    st.write(f"ì‚¬ìš©ì í”¼ë“œë°± ë°˜ì˜: \"{user_feedback[:60]}{'...' if len(user_feedback) > 60 else ''}\"")
                introduction = orch.generate_introduction(
                    st.session_state.topic_analysis,
                    st.session_state.reference_pool,
                    st.session_state.landscape,
                    writing_strategy=st.session_state.get("writing_strategy"),
                    evaluation_feedback=st.session_state.get("evaluation_result"),
                    unsupported_claims=[],
                    user_feedback=user_feedback,
                    current_introduction=st.session_state.introduction_text,
                )
                queries = []
                new_papers = []
                comp_gaps = []
            else:
                # Step 2: Generate supplementary queries (reuse cache if available)
                # Merge claims + completeness gaps for query generation
                all_items_for_queries = list(claims) + orch._completeness_gaps_as_claims(comp_gaps)
                cached_queries = st.session_state.get("evolution_queries", [])
                if cached_queries:
                    queries = cached_queries
                    st.write(f"ìºì‹œëœ ë³´ì¶© ì¿¼ë¦¬ ì‚¬ìš©: {len(queries)}ê°œ")
                else:
                    st.write("ë³´ì¶© ì¿¼ë¦¬ ìƒì„± ì¤‘...")
                    queries = orch.generate_supplementary_queries(
                        all_items_for_queries, st.session_state.topic_analysis
                    )
                    st.write(f"  {len(queries)}ê°œ ë³´ì¶© ì¿¼ë¦¬ ìƒì„±")

                # Step 3: Search PubMed
                st.write("PubMed ë³´ì¶© ê²€ìƒ‰ ì¤‘...")
                new_papers = orch.run_supplementary_search(
                    queries, st.session_state.paper_pool
                )
                st.write(f"  {len(new_papers)}í¸ ìƒˆ ë…¼ë¬¸ ë°œê²¬")

                # Step 4: Expand reference pool (preserve cited papers)
                if new_papers:
                    st.write("Reference pool í™•ì¥ ì¤‘...")
                    st.session_state.paper_pool = st.session_state.paper_pool + new_papers
                    new_ref_pool = orch.expand_reference_pool(
                        st.session_state.reference_pool,
                        new_papers,
                        st.session_state.landscape,
                        current_introduction=st.session_state.introduction_text,
                    )
                    old_size = len(st.session_state.reference_pool)
                    st.session_state.reference_pool = new_ref_pool
                    st.write(f"  Reference pool: {old_size} -> {len(new_ref_pool)}í¸")

                    # Refresh landscape if new papers exceed 20% of pool
                    if len(new_papers) > len(new_ref_pool) * 0.2:
                        st.write("  Landscape ê°±ì‹  ì¤‘ (ìƒˆ ë…¼ë¬¸ ë¹„ìœ¨ > 20%)...")
                        st.session_state.landscape = orch.intro_generator.step_analyze_landscape(
                            st.session_state.paper_pool, st.session_state.current_topic
                        )

                # Step 5: Regenerate introduction with feedback
                if user_feedback:
                    st.write(f"ì‚¬ìš©ì í”¼ë“œë°± ë°˜ì˜: \"{user_feedback[:60]}{'...' if len(user_feedback) > 60 else ''}\"")
                st.write("Introduction ì¬ì‘ì„± ì¤‘...")
                introduction = orch.generate_introduction(
                    st.session_state.topic_analysis,
                    st.session_state.reference_pool,
                    st.session_state.landscape,
                    writing_strategy=st.session_state.get("writing_strategy"),
                    evaluation_feedback=st.session_state.get("evaluation_result"),
                    unsupported_claims=claims,
                    user_feedback=user_feedback,
                    current_introduction=st.session_state.introduction_text,
                )
            # Validate citation range and renumber
            introduction = PipelineOrchestrator.validate_citation_range(
                introduction, len(st.session_state.reference_pool)
            )
            introduction, st.session_state.reference_pool = \
                PipelineOrchestrator.renumber_citations(
                    introduction, st.session_state.reference_pool
                )
            st.session_state.introduction_text = introduction

        # Record evolution details for history
        fallback = not claims and not comp_gaps
        st.session_state.setdefault("evolution_details", []).append({
            "iteration": iteration,
            "claims_found": len(claims),
            "claims_list": claims[:5],
            "completeness_gaps_found": len(comp_gaps),
            "completeness_gaps_list": comp_gaps[:5],
            "queries_used": queries,
            "new_papers_found": len(new_papers),
            "user_feedback": user_feedback,
            "fallback": fallback,
        })

        # Clear cached claims/queries/feedback for next iteration
        st.session_state.evolution_claims = []
        st.session_state.evolution_completeness_gaps = []
        st.session_state.evolution_queries = []
        st.session_state.user_evolution_feedback = ""

        st.session_state.pipeline_iteration = iteration
        st.session_state.pipeline_state = "EVALUATING"
        st.rerun()

    except Exception as e:
        st.error(f"ìë™ ê°œì„  ì‹¤íŒ¨: {str(e)}")
        logger.error(f"Self-evolution error: {e}", exc_info=True)
        st.session_state.pipeline_state = "COMPLETE"
        st.rerun()


def render_complete_state(reference_style: str = "APA"):
    """COMPLETE: Show final results"""
    st.markdown("## ğŸ“ ìµœì¢… ê²°ê³¼")

    # Build final result for display & history
    result = _build_final_result()
    st.session_state.generation_result = result

    # Metrics
    col1, col2, col3 = st.columns(3)
    with col1:
        st.metric("ìˆ˜ì§‘ëœ ë…¼ë¬¸", len(st.session_state.paper_pool))
    with col2:
        st.metric("ì„ ë³„ëœ ë…¼ë¬¸", len(st.session_state.reference_pool))
    with col3:
        iterations = st.session_state.pipeline_iteration
        label = "Draft" if iterations == 0 else f"Draft + {iterations} Rev"
        st.metric("Iterations", label)

    # Iteration history tabs with version selection
    history = st.session_state.iteration_history
    if len(history) > 1:
        st.markdown("### ë²„ì „ ë¹„êµ")
        tab_names = [h["label"] for h in history]
        tabs = st.tabs(tab_names)
        evolution_details = st.session_state.get("evolution_details", [])
        for tab_idx, (tab, h) in enumerate(zip(tabs, history)):
            with tab:
                eval_data = h.get("evaluation", {})
                overall = eval_data.get("overall_score", "?")
                factual = eval_data.get("scores", {}).get("factual_accuracy", "?")
                st.write(f"**ì¢…í•© ì ìˆ˜:** {overall}/10 | **Factual accuracy:** {factual}/10")
                st.markdown(h["introduction"])

                # Show evolution details for revision iterations
                iter_num = h.get("iteration", 0)
                matching_details = [
                    d for d in evolution_details
                    if d.get("iteration") == iter_num
                ]
                if matching_details:
                    detail = matching_details[0]
                    with st.expander("ê°œì„  ìƒì„¸ ì •ë³´", expanded=False):
                        st.write(f"**ë¯¸ì§€ì§€ í´ë ˆì„:** {detail.get('claims_found', 0)}ê°œ")
                        claims_list = detail.get("claims_list", [])
                        if claims_list:
                            for ci, cl in enumerate(claims_list, 1):
                                st.write(f"  {ci}. {cl.get('claim', '')[:100]}")
                        st.write(f"**ë³´ì¶© ê²€ìƒ‰ ì¿¼ë¦¬:** {len(detail.get('queries_used', []))}ê°œ")
                        for qi, q in enumerate(detail.get("queries_used", []), 1):
                            st.write(f"  {qi}. `{q}`")
                        st.write(f"**ìƒˆë¡œ ë°œê²¬ëœ ë…¼ë¬¸:** {detail.get('new_papers_found', 0)}í¸")
                        uf = detail.get("user_feedback", "")
                        if uf:
                            st.write(f"**ì‚¬ìš©ì í”¼ë“œë°±:** {uf}")

                # Version selection button
                is_current = (st.session_state.introduction_text == h["introduction"])
                if is_current:
                    st.success("í˜„ì¬ ì„ íƒëœ ë²„ì „ì…ë‹ˆë‹¤")
                else:
                    if st.button(f"ì´ ë²„ì „ ì„ íƒ ({h['label']})", key=f"select_version_{tab_idx}"):
                        st.session_state.introduction_text = h["introduction"]
                        st.session_state.evaluation_result = h.get("evaluation", {})
                        st.rerun()
    elif history:
        # Single version â€” show it directly
        pass

    # Display final introduction
    st.markdown("---")
    st.markdown("### Introduction")
    st.markdown(st.session_state.introduction_text)

    # References
    st.markdown("### ğŸ“– References")
    ref_pool = st.session_state.reference_pool
    if ref_pool:
        formatted_refs = _format_references_by_style(ref_pool, reference_style)
        for ref in formatted_refs:
            st.markdown(ref)

    # Evaluation results
    evaluation = st.session_state.evaluation_result
    if evaluation:
        display_self_evaluation_results(evaluation)
    else:
        st.warning("í’ˆì§ˆ í‰ê°€ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

    # Auto fact-check results
    fact_result = st.session_state.get("fact_check_result")
    if fact_result:
        st.markdown("---")
        st.markdown("## ìë™ íŒ©íŠ¸ì²´í¬ ê²°ê³¼")
        fc_accuracy = fact_result.get("overall_accuracy", "UNKNOWN")
        fc_issues = fact_result.get("issues", [])
        col1, col2 = st.columns(2)
        with col1:
            st.metric("ì •í™•ë„", fc_accuracy)
        with col2:
            st.metric("ë°œê²¬ëœ ì´ìŠˆ", len(fc_issues))

        # Claim-citation mapping details
        claim_mapping = fact_result.get("claim_mapping", {})
        claim_mappings = claim_mapping.get("claim_mappings", [])
        if claim_mappings:
            with st.expander(f"Claim-Citation ë§¤í•‘ ê²€ì¦ ({len(claim_mappings)}ê°œ claim)", expanded=False):
                for cm in claim_mappings:
                    supported = cm.get("is_supported", True)
                    icon = "+" if supported else "X"
                    claim_text = cm.get("claim", "")[:120]
                    st.write(f"{icon} **{claim_text}**{'...' if len(cm.get('claim', '')) > 120 else ''}")
                    if not supported and cm.get("issue"):
                        st.write(f"   Issue: {cm['issue']}")

        numerical_mismatches = claim_mapping.get("numerical_mismatches", [])
        if numerical_mismatches:
            with st.expander(f"ìˆ˜ì¹˜ ë¶ˆì¼ì¹˜ ({len(numerical_mismatches)}ê±´)", expanded=False):
                for nm in numerical_mismatches:
                    severity = nm.get("severity", "minor").upper()
                    st.write(f"[{severity}] Claimed: {nm.get('claimed_value', '?')} vs Actual: {nm.get('actual_value', '?')}")
                    st.write(f"  {nm.get('claim', '')[:100]}")

        if fc_issues:
            with st.expander(f"íŒ©íŠ¸ì²´í¬ ì´ìŠˆ ìƒì„¸ ({len(fc_issues)}ê±´)", expanded=False):
                for issue in fc_issues:
                    st.write(f"- **{issue.get('type', '')}**: {issue.get('description', '')}")

    # Landscape details
    with st.expander("ğŸŒ ë¬¸í—Œ ê²½ê´€ ë¶„ì„", expanded=False):
        _display_landscape(st.session_state.landscape)

    # Topic details
    topic_analysis = st.session_state.topic_analysis
    if topic_analysis:
        with st.expander("ğŸ“‹ íŒŒì‹±ëœ ì£¼ì œ ì •ë³´", expanded=False):
            col1, col2 = st.columns(2)
            with col1:
                st.write(f"**ì§ˆí™˜:** {topic_analysis.get('disease', 'N/A')}")
                st.write(f"**ë°ì´í„° ìœ í˜•:** {topic_analysis.get('data_type', 'N/A')}")
            with col2:
                st.write(f"**ë°©ë²•ë¡ :** {topic_analysis.get('methodology', 'N/A')}")
                st.write(f"**ì˜ˆì¸¡ ëŒ€ìƒ:** {topic_analysis.get('outcome', 'N/A')}")

    # Action buttons
    st.markdown("---")
    col1, col2, col3 = st.columns(3)

    with col1:
        if st.button("ğŸ“‹ Introduction ë³µì‚¬"):
            st.write("ë³µì‚¬ë˜ì—ˆìŠµë‹ˆë‹¤ (ë¸Œë¼ìš°ì € ë³µì‚¬ ê¸°ëŠ¥ ì‚¬ìš©)")

    with col2:
        fc_label = "âœ… íŒ©íŠ¸ì²´í¬ ì¬ì‹¤í–‰" if fact_result else "âœ… íŒ©íŠ¸ì²´í¬ ì‹¤í–‰"
        if st.button(fc_label, key="factcheck_btn"):
            run_fact_check(result)

    with col3:
        if st.button("ğŸ”„ ìƒˆ ì£¼ì œ ì‹œì‘", key="new_topic_btn"):
            # Save to history before reset
            if st.session_state.current_topic and result:
                st.session_state.history.append({
                    "topic": st.session_state.current_topic,
                    "result": result
                })
            reset_pipeline()
            st.rerun()

    # Manual revision section
    st.markdown("---")
    st.markdown("### ìˆ˜ì •í•˜ê³  ì‹¶ì€ ë¶€ë¶„ì´ ìˆìœ¼ì‹ ê°€ìš”?")

    revision_request = st.text_area(
        "ìˆ˜ì • ìš”ì²­ ì…ë ¥",
        placeholder="ì˜ˆ: 'ë‘ ë²ˆì§¸ ë¬¸ë‹¨ì„ ë” ë³´ê°•í•´ì¤˜' ë˜ëŠ” 'ì „ì²´ì ìœ¼ë¡œ í†¤ì„ ë” formalí•˜ê²Œ ë°”ê¿”ì¤„ë˜'",
        height=80,
        key="revision_request"
    )

    if st.button("ìˆ˜ì • ëª…ë ¹ ì‹¤í–‰", key="run_revision_btn"):
        if not revision_request:
            st.warning("ìˆ˜ì • ìš”ì²­ì„ ì…ë ¥í•˜ì„¸ìš”")
        else:
            run_revision(
                st.session_state.introduction_text,
                revision_request,
                st.session_state.api_key_stored,
                st.session_state.model_stored,
                st.session_state.reference_pool
            )


def _build_final_result() -> dict:
    """Build a result dictionary from current pipeline state"""
    from utils.pubmed_utils import format_citation_vancouver
    references = []
    for i, article in enumerate(st.session_state.reference_pool, 1):
        citation = format_citation_vancouver(article, i)
        references.append(citation)

    return {
        "introduction": st.session_state.introduction_text,
        "references": references,
        "articles_used": st.session_state.reference_pool,
        "parsing_result": st.session_state.topic_analysis or {},
        "landscape": st.session_state.landscape,
        "paper_pool_size": len(st.session_state.paper_pool),
        "reference_pool_size": len(st.session_state.reference_pool),
        "evaluation": st.session_state.evaluation_result,
    }


# ------------------------------------------------------------------
# Shared display helpers
# ------------------------------------------------------------------

def _display_landscape(landscape: dict):
    """Display landscape analysis details"""
    field_overview = landscape.get("field_overview", "")
    if field_overview:
        st.markdown("**ë¶„ì•¼ ê°œìš”:**")
        st.markdown(field_overview)

    key_findings = landscape.get("key_findings", [])
    if key_findings:
        st.markdown(f"\n**í•µì‹¬ ë°œê²¬ì‚¬í•­ ({len(key_findings)}ê°œ):**")
        for i, finding in enumerate(key_findings, 1):
            st.markdown(f"{i}. {finding}")

    knowledge_gaps = landscape.get("knowledge_gaps", [])
    if knowledge_gaps:
        st.markdown(f"\n**ë¯¸ì¶©ì¡± ì—°êµ¬ í•„ìš” ë¶„ì•¼ ({len(knowledge_gaps)}ê°œ):**")
        for i, gap in enumerate(knowledge_gaps, 1):
            st.markdown(f"{i}. {gap}")

    trends = landscape.get("methodological_trends", [])
    if trends:
        st.markdown(f"\n**ë°©ë²•ë¡ ì  ë™í–¥ ({len(trends)}ê°œ):**")
        for i, trend in enumerate(trends, 1):
            st.markdown(f"{i}. {trend}")

    controversies = landscape.get("controversies_or_debates", [])
    if controversies:
        st.markdown(f"\n**ë…¼ë€/ë¯¸í•´ê²° ìŸì  ({len(controversies)}ê°œ):**")
        for i, c in enumerate(controversies, 1):
            st.markdown(f"{i}. {c}")

    underexplored = landscape.get("underexplored_areas", [])
    if underexplored:
        st.markdown(f"\n**ë¯¸íƒêµ¬ ì˜ì—­ ({len(underexplored)}ê°œ):**")
        for i, area in enumerate(underexplored, 1):
            st.markdown(f"{i}. {area}")


def display_self_evaluation_results(evaluation: dict):
    """Display self-evaluation results"""
    st.markdown("---")
    st.markdown("## ğŸ¯ ìë™ í’ˆì§ˆ í‰ê°€")

    overall_score = evaluation.get("overall_score", 0)
    passed = evaluation.get("passed", False)

    col1, col2, col3 = st.columns(3)
    with col1:
        st.metric("ì¢…í•© ì ìˆ˜", f"{overall_score}/10")
    with col2:
        status = "í•©ê²©" if passed else "ê²€í†  í•„ìš”"
        st.markdown(f"**ìƒíƒœ:** {status}")
    with col3:
        improvement_count = len(evaluation.get("improvements", []))
        st.metric("ê°œì„  í•­ëª©", improvement_count)

    # Detailed scores
    st.markdown("### ì„¸ë¶€ ì ìˆ˜")
    scores = evaluation.get("scores", {})
    cols = st.columns(5)
    criteria = list(scores.items())
    for i, (criterion, score) in enumerate(criteria):
        col_idx = i % 5
        with cols[col_idx]:
            if score >= 8:
                emoji = "âœ…"
            elif score >= 7:
                emoji = "O"
            elif score >= 5:
                emoji = "â–³"
            else:
                emoji = "X"
            st.write(f"{emoji} **{criterion.replace('_', ' ').title()}**")
            st.write(f"ì ìˆ˜: {score}/10")

    # Feedback
    feedback = evaluation.get("feedback", {})
    improvements = evaluation.get("improvements", [])

    st.markdown("### í‰ê°€ í”¼ë“œë°±")
    for criterion_key, fb_text in feedback.items():
        criterion_label = criterion_key.replace("_", " ").title()
        criterion_score = scores.get(criterion_key, "?")
        with st.expander(f"{criterion_label} (ì ìˆ˜: {criterion_score}/10)", expanded=False):
            st.markdown(f"**í”¼ë“œë°±:** {fb_text}")
            for imp in improvements:
                if imp["criterion"] == criterion_key:
                    st.markdown(f"\n**ê°œì„  ì œì•ˆ:** {imp.get('improvement', '')}")
                    break

    if not improvements:
        st.success("âœ… ëª¨ë“  ê¸°ì¤€ì—ì„œ 7ì  ì´ìƒì„ íšë“í–ˆìŠµë‹ˆë‹¤!")


def run_fact_check(generation_result: dict):
    """Run fact-checking on generated introduction"""
    st.markdown("### ğŸ” íŒ©íŠ¸ì²´í¬")

    with st.spinner("íŒ©íŠ¸ì²´í¬ ì§„í–‰ ì¤‘..."):
        try:
            orch = get_orchestrator()
            if orch:
                fact_checker = FactChecker(
                    llm_client=orch.llm_client,
                    pubmed_client=orch.pubmed_client,
                )
            else:
                fact_checker = FactChecker()
            check_result = fact_checker.verify_introduction(
                generation_result.get("introduction", ""),
                generation_result.get("articles_used", [])
            )

            st.session_state.fact_check_result = check_result

            accuracy = check_result.get("overall_accuracy", "UNKNOWN")
            st.metric("ì „ì²´ ì •í™•ë„", accuracy, delta="Verified")

            issues = check_result.get("issues", [])
            if issues:
                st.warning(f"âš ï¸ {len(issues)}ê°œì˜ ì ì¬ì  ë¬¸ì œ ë°œê²¬")
                for issue in issues:
                    st.write(f"- **{issue.get('type')}**: {issue.get('description', '')}")
            else:
                st.success("âœ… ëª¨ë“  ì¸ìš©ì´ ê²€ì¦ë˜ì—ˆìŠµë‹ˆë‹¤")

            st.info(check_result.get("summary", ""))

        except Exception as e:
            st.error(f"íŒ©íŠ¸ì²´í¬ ì‹¤íŒ¨: {str(e)}")
            logger.error(f"Fact check error: {e}", exc_info=True)


def run_revision(
    current_intro: str,
    revision_request: str,
    api_key: str,
    model: str,
    articles_used: list
):
    """Run revision on introduction"""
    if not revision_request.strip():
        st.warning("ìˆ˜ì • ìš”ì²­ì„ ì…ë ¥í•˜ì„¸ìš”")
        return

    try:
        from prompts.revision import get_revision_prompt

        st.markdown("### ìˆ˜ì • ì§„í–‰ ì¤‘...")
        progress_bar = st.progress(0)
        status_text = st.empty()

        status_text.write("Revision í”„ë¡¬í”„íŠ¸ ìƒì„± ì¤‘...")
        progress_bar.progress(0.3)

        provider = st.session_state.get("provider_stored", "openai")
        llm_client = get_llm_client(
            provider=provider,
            api_key=api_key,
            model=model,
            azure_endpoint=st.session_state.get("azure_endpoint_stored") or None,
            api_version=st.session_state.get("azure_api_version_stored") or None,
            base_model=st.session_state.get("azure_base_model_stored") or None,
        )
        system_prompt, user_prompt = get_revision_prompt(
            current_intro, revision_request, articles_used
        )

        status_text.write("LLMìœ¼ë¡œ ìˆ˜ì • ìƒì„± ì¤‘...")
        progress_bar.progress(0.7)

        revised_intro = llm_client.generate(
            prompt=user_prompt,
            system_prompt=system_prompt,
            temperature=0.7,
            max_tokens=2000,
            reasoning_effort="high",
        )

        progress_bar.progress(1.0)

        from core.pipeline_orchestrator import _normalize_paragraph_breaks
        revised_intro = _normalize_paragraph_breaks(revised_intro)
        revised_intro = PipelineOrchestrator.validate_citation_range(
            revised_intro, len(st.session_state.get("reference_pool", []))
        )
        revised_intro, ref_pool = PipelineOrchestrator.renumber_citations(
            revised_intro, st.session_state.get("reference_pool", [])
        )
        if ref_pool:
            st.session_state.reference_pool = ref_pool
        st.session_state.introduction_text = revised_intro
        st.session_state.current_intro = revised_intro
        st.success("âœ… ìˆ˜ì • ì™„ë£Œ!")

        st.markdown("### ìˆ˜ì •ëœ Introduction")
        st.markdown(revised_intro)

        if st.button("ìˆ˜ì • ë²„ì „ íŒ©íŠ¸ì²´í¬", key="revised_factcheck"):
            rev_orch = get_orchestrator()
            if rev_orch:
                fact_checker = FactChecker(
                    llm_client=rev_orch.llm_client,
                    pubmed_client=rev_orch.pubmed_client,
                )
            else:
                fact_checker = FactChecker()
            check_result = fact_checker.verify_introduction(revised_intro, articles_used)
            accuracy = check_result.get("overall_accuracy", "UNKNOWN")
            st.metric("ì •í™•ë„", accuracy)
            issues = check_result.get("issues", [])
            if issues:
                st.warning(f"âš ï¸ {len(issues)}ê°œ í•­ëª© í™•ì¸ í•„ìš”")
            else:
                st.success("âœ… íŒ©íŠ¸ì²´í¬ ì™„ë£Œ")

    except Exception as e:
        st.error(f"ìˆ˜ì • ì‹¤íŒ¨: {str(e)}")
        logger.error(f"Revision error: {e}", exc_info=True)


def _format_references_by_style(articles: list, style: str = "APA") -> list:
    """Format article list into citation strings"""
    refs = []
    for i, article in enumerate(articles, 1):
        authors = article.get("authors", [])
        title = article.get("title", "Untitled")
        journal = article.get("journal", "")
        journal_iso = article.get("journal_iso", journal)
        year = article.get("pub_year", "n.d.")
        pmid = article.get("pmid", "")
        doi = article.get("doi", "")

        if style == "APA":
            apa_authors = []
            for a in authors[:6]:
                parts = a.split()
                if len(parts) >= 2:
                    last = parts[0]
                    initials = ". ".join(p[0] + "." for p in parts[1:] if p)
                    apa_authors.append(f"{last}, {initials}")
                else:
                    apa_authors.append(a)
            if len(authors) > 6:
                author_str = ", ".join(apa_authors) + ", ... et al."
            elif len(apa_authors) > 1:
                author_str = ", ".join(apa_authors[:-1]) + ", & " + apa_authors[-1]
            elif apa_authors:
                author_str = apa_authors[0]
            else:
                author_str = "Unknown"
            doi_part = f" https://doi.org/{doi}" if doi else ""
            ref = f"{i}. {author_str} ({year}). {title}. *{journal_iso}*.{doi_part} PMID: {pmid}"

        elif style == "Vancouver":
            van_authors = []
            for a in authors[:3]:
                van_authors.append(a)
            if len(authors) > 3:
                author_str = ", ".join(van_authors) + " et al."
            else:
                author_str = ", ".join(van_authors)
            ref = f"{i}. {author_str}. {title}. {journal_iso}. {year}. PMID: {pmid}."

        else:  # AMA
            ama_authors = []
            for a in authors[:3]:
                ama_authors.append(a)
            if len(authors) > 3:
                author_str = ", ".join(ama_authors) + ", et al."
            else:
                author_str = ", ".join(ama_authors)
            ref = f"{i}. {author_str}. {title}. *{journal_iso}*. {year}. PMID: {pmid}."

        refs.append(ref)
    return refs


# ------------------------------------------------------------------
# Main
# ------------------------------------------------------------------

def main():
    """Main application flow â€” state machine dispatcher"""
    initialize_session()

    api_key, model, reference_style = setup_sidebar()

    display_header()
    st.markdown("---")

    state = st.session_state.pipeline_state

    if state == "IDLE":
        render_idle_state()
    elif state == "PARSING":
        render_parsing_state()
    elif state == "CONFIRM_QUERIES":
        render_confirm_queries_state()
    elif state == "RESEARCHING":
        render_researching_state()
    elif state == "CONFIRM_STRATEGY":
        render_confirm_strategy_state()
    elif state == "GENERATING":
        render_generating_state()
    elif state == "EVALUATING":
        render_evaluating_state()
    elif state == "EVALUATION_FAILED":
        render_evaluation_failed_state()
    elif state == "CONFIRM_EVOLUTION":
        render_confirm_evolution_state()
    elif state == "SELF_EVOLVING":
        render_self_evolving_state()
    elif state == "COMPLETE":
        render_complete_state(reference_style=reference_style)
    else:
        st.error(f"Unknown pipeline state: {state}")
        reset_pipeline()
        st.rerun()


if __name__ == "__main__":
    main()
